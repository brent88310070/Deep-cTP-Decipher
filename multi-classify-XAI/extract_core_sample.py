import numpy as np
import pandas as pd
from sklearn.metrics import silhouette_samples
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys

# ==========================================
# Configuration
# ==========================================
# Path to the CSV containing cluster labels (generated by subclustering.py)
INPUT_CSV = "../sub-clustering/consensus_labels.csv"

# Path to the pre-calculated consensus matrix (.npz file)
# NOTE: Ensure this file exists. (generated by subclustering.py)
# You may need to adjust this path depending on where your matrix file is located.
CONSENSUS_MATRIX_PATH = "../sub-clustering/consensus_matrix.npz" 

# Output paths
OUTPUT_CSV = "./consensus_labels_core.csv"
PLOT_FILENAME = "silhouette_distribution.png"

# Filter parameters
TOP_N_PERCENT = 0.5
MIN_THRESHOLD = 0.0

# ==========================================
# Helper Functions
# ==========================================

def filter_core_samples(df, score_col='silhouette_score', cluster_col='cluster', top_n_percent=0.75, min_threshold=0.0):
    """
    Logic for filtering Core Samples:
    1. Must be greater than the minimum threshold (min_threshold, usually 0).
    2. Within that cluster, rank in the top N% (top_n_percent) by score.
    """
    core_samples = []

    for cluster_id, group in df.groupby(cluster_col):
        # Filter 1: Threshold
        valid_samples = group[group[score_col] > min_threshold].copy()
        
        # Filter 2: Top N%
        n_samples = int(len(group) * top_n_percent)
        
        # Ensure we keep at least some samples if valid ones exist but n_samples calculates to 0
        if n_samples < 1 and len(valid_samples) > 0:
            n_samples = len(valid_samples)
        elif len(valid_samples) == 0:
            n_samples = 0

        if n_samples > 0:
            top_samples = valid_samples.nlargest(n_samples, score_col)
            core_samples.append(top_samples)

    if core_samples:
        return pd.concat(core_samples)
    else:
        return pd.DataFrame(columns=df.columns)

def plot_silhouette_distribution(df, output_filename):
    """
    Generates a violin plot of silhouette scores per cluster and saves it.
    """
    unique_clusters = df['cluster'].unique()
    n_clusters = len(unique_clusters)
    
    # Adjust palette size dynamically
    if n_clusters <= 10:
        palette = sns.color_palette("Set2", n_clusters)
    else:
        palette = sns.color_palette("viridis", n_clusters)

    plt.figure(figsize=(10, 6))
    sns.violinplot(
        x='cluster', 
        y='silhouette_score', 
        data=df, 
        palette=palette, 
        inner="quartile",
        hue='cluster',
        legend=False
    )
    plt.axhline(y=0, color='r', linestyle='--', label='Threshold 0')
    plt.title('Silhouette Score Distribution per Cluster')
    plt.xlabel('Cluster Label')
    plt.ylabel('Silhouette Coefficient')
    plt.legend()
    
    print(f"Saving silhouette distribution plot to {output_filename}...")
    plt.savefig(output_filename)
    plt.close()

# ==========================================
# Main Execution
# ==========================================
if __name__ == "__main__":
    # 1. Check file existence
    if not os.path.exists(INPUT_CSV):
        print(f"Error: Input CSV {INPUT_CSV} not found.")
        sys.exit(1)
        
    if not os.path.exists(CONSENSUS_MATRIX_PATH):
        print(f"Error: Consensus matrix file {CONSENSUS_MATRIX_PATH} not found.")
        print("Please ensure you have the consensus matrix .npz file and update CONSENSUS_MATRIX_PATH in the script.")
        sys.exit(1)

    print("Loading data...")
    consensus_df = pd.read_csv(INPUT_CSV)
    
    # Load consensus matrix
    # Assuming the npz file has a key named 'consensus_matrix'
    try:
        data = np.load(CONSENSUS_MATRIX_PATH)
        if "consensus_matrix" in data:
            consensus_matrix = data["consensus_matrix"]
        else:
            # Fallback if keys are different, try the first array
            key = data.files[0]
            consensus_matrix = data[key]
            print(f"Warning: 'consensus_matrix' key not found, using '{key}' instead.")
    except Exception as e:
        print(f"Error loading numpy file: {e}")
        sys.exit(1)

    # 2. Calculate Silhouette Scores
    print("Calculating silhouette scores...")
    # Distance matrix = 1 - Similarity (Consensus)
    distance_matrix = 1 - consensus_matrix
    np.fill_diagonal(distance_matrix, 0)
    
    cluster_labels = consensus_df['cluster'].values
    
    # Validate dimensions
    if consensus_matrix.shape[0] != len(consensus_df):
        print(f"Error: Matrix dimension ({consensus_matrix.shape[0]}) does not match DataFrame length ({len(consensus_df)}).")
        sys.exit(1)

    sil_values = silhouette_samples(distance_matrix, cluster_labels, metric='precomputed')
    consensus_df['silhouette_score'] = sil_values

    # 3. Plot Distribution
    plot_silhouette_distribution(consensus_df, PLOT_FILENAME)

    # 4. Filter Core Samples
    print(f"Filtering core samples (Top {TOP_N_PERCENT:.0%}, Threshold > {MIN_THRESHOLD})...")
    core_df = filter_core_samples(
        consensus_df, 
        top_n_percent=TOP_N_PERCENT, 
        min_threshold=MIN_THRESHOLD
    )

    print("-" * 30)
    print(f"Original sample count: {len(consensus_df)}")
    print(f"Core sample count:     {len(core_df)}")
    print(f"Retention ratio:       {len(core_df)/len(consensus_df):.2%}")
    print("-" * 30)

    print("\nCluster counts after filtering:")
    print(core_df['cluster'].value_counts().sort_index())

    # 5. Export Results
    # Mark boolean flag in original dataframe
    core_indices = core_df.index
    consensus_df['is_core_sample'] = consensus_df.index.isin(core_indices)
    
    print(f"Saving results to {OUTPUT_CSV}...")
    consensus_df.to_csv(OUTPUT_CSV, index=False)
    print("Done.")